import json
import os
from dotenv import load_dotenv
from datasets import Dataset

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)

from langchain_anthropic import ChatAnthropic

# Configuration du LLM pour Ragas
llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")

# Charger les variables d'environnement
load_dotenv()

# Charger le jeu de test annotÃ©
print("ğŸ“‚ Chargement du jeu de test...")
with open("data/evaluation/test_set.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

print(f"âœ… {len(test_data['test_cases'])} questions chargÃ©es\n")

# Importer le systÃ¨me RAG
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag.langchain_rag import PulsEventsRAG

# Initialiser le RAG
print("ğŸš€ Initialisation du systÃ¨me RAG...")
rag = PulsEventsRAG()
print("âœ… RAG prÃªt !\n")

# PrÃ©parer les listes pour Ragas
questions = []
answers = []
contexts = []
ground_truths = []

print("ğŸ”„ Interrogation du RAG pour chaque question...\n")
for i, test_case in enumerate(test_data['test_cases'], 1):
    question = test_case['question']
    ground_truth = test_case['ground_truth']
    
    print(f"Question {i}/{len(test_data['test_cases'])}: {question}")
    
    # Interroger le RAG
    result = rag.ask(question)
    
    # Extraire la rÃ©ponse et les sources
    answer = result['answer']
    retrieved_docs = result['sources']
    
    # Construire la liste des contextes (texte des documents rÃ©cupÃ©rÃ©s)
    context_list = retrieved_docs
    
    # Ajouter aux listes
    questions.append(question)
    answers.append(answer)
    contexts.append(context_list)
    ground_truths.append(ground_truth)
    
    print(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e\n")

# CrÃ©er le dataset au format Ragas
print("ğŸ“Š CrÃ©ation du dataset pour Ragas...")
evaluation_dataset = Dataset.from_dict({
    "question": questions,
    "answer": answers,
    "contexts": contexts,
    "ground_truth": ground_truths
})
print(f"âœ… Dataset crÃ©Ã© avec {len(evaluation_dataset)} exemples\n")

# Lancer l'Ã©valuation Ragas
print("ğŸ¯ Lancement de l'Ã©valuation Ragas...")
print("â³ Cela peut prendre 2-3 minutes (appels LLM pour calculer les mÃ©triques)...\n")

os.environ["OPENAI_API_KEY"] = "dummy" # Bloquer OpenAI

results = evaluate(
    evaluation_dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ],
    llm=llm,
    embeddings=None
)

print("âœ… Ã‰valuation terminÃ©e !\n")

# Afficher les rÃ©sultats
print("=" * 50)
print("ğŸ“Š RÃ‰SULTATS DE L'Ã‰VALUATION RAGAS")
print("=" * 50)
print(f"\nğŸ¯ Faithfulness:       {results.to_pandas()['faithfulness'].mean():.3f}")
print(f"ğŸ¯ Answer Relevancy:   {results.to_pandas()['answer_relevancy'].mean():.3f}")
print(f"ğŸ¯ Context Precision:  {results.to_pandas()['context_precision'].mean():.3f}")
print(f"ğŸ¯ Context Recall:     {results.to_pandas()['context_recall'].mean():.3f}")
print("\n" + "=" * 50)

# Sauvegarder les rÃ©sultats en JSON
output_path = "data/evaluation/ragas_results.json"
results_dict = {
    "faithfulness": float(results.to_pandas()['faithfulness'].mean()),
    "answer_relevancy": float(results.to_pandas()['answer_relevancy'].mean()),
    "context_precision": float(results.to_pandas()['context_precision'].mean()),
    "context_recall": float(results.to_pandas()['context_recall'].mean()),
    "num_questions": len(test_data['test_cases'])
}

with open(output_path, "w", encoding="utf-8") as f:
    json.dump(results_dict, f, indent=2, ensure_ascii=False)

print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s dans : {output_path}")
print("\nâœ… Ã‰valuation MVP7 terminÃ©e avec succÃ¨s !")