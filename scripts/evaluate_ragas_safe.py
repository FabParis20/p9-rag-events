import json
import os
import sys
from dotenv import load_dotenv
from datasets import Dataset
from langchain_anthropic import ChatAnthropic

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall
)

# Ajouter le chemin parent
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from rag.langchain_rag import PulsEventsRAG

# Charger les variables d'environnement
load_dotenv()

# Configuration du LLM pour Ragas
llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")
os.environ["OPENAI_API_KEY"] = "dummy"

# Fichiers de sauvegarde
TEMP_FILE = "data/evaluation/temp_rag_responses.json"
RESULTS_FILE = "data/evaluation/ragas_results.json"

# Charger le jeu de test
print("ğŸ“‚ Chargement du jeu de test...")
with open("data/evaluation/test_set.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)
print(f"âœ… {len(test_data['test_cases'])} questions chargÃ©es\n")

# VÃ©rifier si on a dÃ©jÃ  des rÃ©ponses sauvegardÃ©es
if os.path.exists(TEMP_FILE):
    print("ğŸ’¾ Fichier de sauvegarde trouvÃ© ! Chargement des rÃ©ponses...")
    with open(TEMP_FILE, "r", encoding="utf-8") as f:
        saved_data = json.load(f)
    questions = saved_data["questions"]
    answers = saved_data["answers"]
    contexts = saved_data["contexts"]
    ground_truths = saved_data["ground_truths"]
    print(f"âœ… {len(questions)} rÃ©ponses chargÃ©es depuis la sauvegarde\n")
else:
    # Initialiser le RAG
    print("ğŸš€ Initialisation du systÃ¨me RAG...")
    rag = PulsEventsRAG()
    print("âœ… RAG prÃªt !\n")

    # Interroger le RAG
    questions = []
    answers = []
    contexts = []
    ground_truths = []

    print("ğŸ”„ Interrogation du RAG pour chaque question...\n")
    for i, test_case in enumerate(test_data['test_cases'], 1):
        question = test_case['question']
        ground_truth = test_case['ground_truth']
        
        print(f"Question {i}/{len(test_data['test_cases'])}: {question}")
        
        result = rag.ask(question)
        answer = result['answer']
        context_list = result['sources']
        
        questions.append(question)
        answers.append(answer)
        contexts.append(context_list)
        ground_truths.append(ground_truth)
        
        # Sauvegarder aprÃ¨s chaque question
        temp_data = {
            "questions": questions,
            "answers": answers,
            "contexts": contexts,
            "ground_truths": ground_truths
        }
        with open(TEMP_FILE, "w", encoding="utf-8") as f:
            json.dump(temp_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e et sauvegardÃ©e\n")

# CrÃ©er le dataset pour Ragas
print("ğŸ“Š CrÃ©ation du dataset pour Ragas...")
evaluation_dataset = Dataset.from_dict({
    "question": questions,
    "answer": answers,
    "contexts": contexts,
    "ground_truth": ground_truths
})
print(f"âœ… Dataset crÃ©Ã© avec {len(evaluation_dataset)} exemples\n")

# Lancer l'Ã©valuation Ragas
print("ğŸ¯ Lancement de l'Ã©valuation Ragas...")
print("â³ Cela peut prendre 2-3 minutes...\n")

results = evaluate(
    evaluation_dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ],
    llm=llm,
    embeddings=None
)

print("âœ… Ã‰valuation terminÃ©e !\n")

# Afficher les rÃ©sultats
print("=" * 50)
print("ğŸ“Š RÃ‰SULTATS DE L'Ã‰VALUATION RAGAS")
print("=" * 50)
print(f"\nğŸ¯ Faithfulness:       {results.to_pandas()['faithfulness'].mean():.3f}")
print(f"ğŸ¯ Answer Relevancy:   {results.to_pandas()['answer_relevancy'].mean():.3f}")
print(f"ğŸ¯ Context Precision:  {results.to_pandas()['context_precision'].mean():.3f}")
print(f"ğŸ¯ Context Recall:     {results.to_pandas()['context_recall'].mean():.3f}")
print("\n" + "=" * 50)

# Sauvegarder les rÃ©sultats
results_dict = {
    "faithfulness": float(results.to_pandas()['faithfulness'].mean()),
    "answer_relevancy": float(results.to_pandas()['answer_relevancy'].mean()),
    "context_precision": float(results.to_pandas()['context_precision'].mean()),
    "context_recall": float(results.to_pandas()['context_recall'].mean()),
    "num_questions": len(test_data['test_cases'])
}

with open(RESULTS_FILE, "w", encoding="utf-8") as f:
    json.dump(results_dict, f, indent=2, ensure_ascii=False)

print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s dans : {RESULTS_FILE}")

# Nettoyer le fichier temporaire
if os.path.exists(TEMP_FILE):
    os.remove(TEMP_FILE)
    print("ğŸ—‘ï¸  Fichier temporaire supprimÃ©")

print("\nâœ… Ã‰valuation MVP7 terminÃ©e avec succÃ¨s !")